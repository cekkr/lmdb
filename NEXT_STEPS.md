- Wire the new `PAIR_SCAN` command into `src/db_slm` so Level 1 context lookups iterate directly over cheetah namespaces; start with `NGramStore`/`Decoder` and snapshot coverage in `cheetah-db/README.md` once â‰¥90% of Top-K fetches skip SQLite entirely.
- Remove the `ColdStorageFlusher` + MariaDB migration path after cheetah streaming lands: delete the `DBSLM_MARIADB_*` settings, drop the flush daemon, and document the cheetah-only archival story in `AI_REFERENCE.md`.
- Re-run a cheetah-only ingest (`DBSLM_BACKEND=cheetah-db python src/train.py datasets/emotion_data.json --ngram-order 3 --eval-interval 2000`) once the adapter no longer queries SQLite, recording decoder latency / Top-K hit rates and command transcripts inside `cheetah-db/README.md`.
- Extend the new scanner with reducer hooks (`PAIR_REDUCE counts`, `PAIR_REDUCE probs`, etc.) so `MKNSmoother.rebuild_all()` can execute entirely via TCP without materializing SQLite temporary tables; draft the RPC contract in `cheetah-db/CONCEPTS.md`.
- After the reducers exist, drop the SQLite metadata bootstrap in `DatabaseEnvironment` (or wrap it behind feature switches) and migrate the metadata that still matters (context dimensions, decode presets) into cheetah-native namespaces.
- Keep `var/eval_logs/quality_retrain_queue.jsonl` under 200 pending entries by scheduling a cheetah-backed retrain drain once it crosses 150 items, using the `queue-drain` preset from `studies/best_commands.md` and logging the throughput in `studies/BENCHMARKS.md`.
