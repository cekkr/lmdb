- Re-run `make smoke-train` (or the equivalent command listed in `studies/best_commands.md#efficient-smoke-train`) with the high-efficiency flags: bump `--eval-interval` to â‰¥5000, set `--eval-samples 1`, raise `--json-chunk-size` to 500+, and enable `--profile-ingest`. Capture the resulting chunk latency/RSS figures in `studies/datasets.md`.
- After the efficient run, export the `var/eval_logs/train-*.json` metrics summary and use it to update `AI_REFERENCE.md` with the new throughput (tokens/s) and probe cadence so the next agent keeps the tuned settings.
- Keep `var/eval_logs/quality_retrain_queue.jsonl` under 200 pending entries by scheduling a focused retrain pass that ingests only that queue once it crosses 150 items, using the `queue-drain` command preset in `studies/best_commands.md`.
