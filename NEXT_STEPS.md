- Wire the new `PAIR_SCAN` command into `src/db_slm` so Level 1 context lookups iterate directly over cheetah namespaces; start with `NGramStore`/`Decoder` and snapshot coverage in `cheetah-db/README.md` once â‰¥90% of Top-K fetches skip SQLite entirely.
- In cheetah-db (and related use in src/db-slm) make a more evident implementation of "context relativism probabilistic tree result". This kind of query should returns not just a key-value, but a series of keys with a probabilistic correlations with the given context. A context could have a dynamic level of depth. In this example a single "word" is represented by an array for tokenization evidence: a context could be [[[12, 45], [43, 21]], [[643, 23]]] or [[[12, 45], [43, 21]],[[643, 23]], [432,21,23], [223, 432, 213, 213]] for a deeper contextualization. In this case, is important to create for cheetah-db an optimal "absolute vectors ordering" approach where different "words" in the same dimension are ordered in a fixed way through the time so it doesn't confuse the db engine in case of words in different order from a query
- Remove the `ColdStorageFlusher` + MariaDB migration path after cheetah streaming lands: delete the `DBSLM_MARIADB_*` settings, drop the flush daemon, and document the cheetah-only archival story in `AI_REFERENCE.md`.
- Re-run a cheetah-only ingest (`DBSLM_BACKEND=cheetah-db python src/train.py datasets/emotion_data.json --ngram-order 3 --eval-interval 2000`) once the adapter no longer queries SQLite, recording decoder latency / Top-K hit rates and command transcripts inside `cheetah-db/README.md`.
- Extend the new scanner with reducer hooks (`PAIR_REDUCE counts`, `PAIR_REDUCE probs`, etc.) so `MKNSmoother.rebuild_all()` can execute entirely via TCP without materializing SQLite temporary tables; draft the RPC contract in `cheetah-db/CONCEPTS.md`.
- After the reducers exist, drop the SQLite metadata bootstrap in `DatabaseEnvironment` (or wrap it behind feature switches) and migrate the metadata that still matters (context dimensions, decode presets) into cheetah-native namespaces.
- Keep `var/eval_logs/quality_retrain_queue.jsonl` under 200 pending entries by scheduling a cheetah-backed retrain drain once it crosses 150 items, using the `queue-drain` preset from `studies/best_commands.md` and logging the throughput in `studies/BENCHMARKS.md`.
