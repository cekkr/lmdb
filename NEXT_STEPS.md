- Run the smoke-train repeat-penalty grid using the new `--decoder-presence-penalty/--decoder-frequency-penalty` overrides: sweep {(0.18, 0.06), (0.22, 0.08), (0.26, 0.10)} across 150k-token ingests and append the resulting `structure_variety`, `common_token_penalty`, and `token_group_share` deltas to `studies/BENCHMARKS.md`.
- With the token-group repetition metric + relaxed `QualityGate.common_token_ceiling` in place, capture the next evaluation batch to confirm the flagged rate drops below 45% while `structure_variety_mean` stays at/above 0.35, and log the before/after snapshot (including `token_group_share_mean` and `quality_score_mean`) in `studies/BENCHMARKS.md`.
- Re-run `make smoke-train` (or the equivalent command listed in `studies/best_commands.md#efficient-smoke-train`) with the high-efficiency flags: bump `--eval-interval` to 5000, set `--eval-samples 1`, raise `--json-chunk-size` to 500+, and enable `--profile-ingest`. Capture the resulting chunk latency/RSS figures in `studies/datasets.md`.
- After the efficient run, export the `var/eval_logs/train-*.json` metrics summary and use it to update `AI_REFERENCE.md` with the new throughput (tokens/s) and probe cadence so the next agent keeps the tuned settings.
- Keep `var/eval_logs/quality_retrain_queue.jsonl` under 200 pending entries by scheduling a focused retrain pass that ingests only that queue once it crosses 150 items, using the `queue-drain` command preset in `studies/best_commands.md`.
