- Run a quick grid on the decoder repeat penalties to raise structure variety without tanking fluency: expose overrides for `DecoderConfig.presence_penalty` / `frequency_penalty` in the smoke-train CLI, sweep {(0.18, 0.06), (0.22, 0.08), (0.26, 0.10)} across 150k-token ingests, and log `structure_variety` plus `common_token_penalty` deltas in `studies/BENCHMARKS.md`.
- Add a token-group repetition metric to `src/db_slm/evaluation.py` (top bi-/tri-gram share inside each candidate), feed it through `SentenceQualityScorer` so `quality_score` scales when group reuse exceeds 30%, then retune `QualityGate.common_token_ceiling` to target <45% flagged rate while keeping `structure_variety_mean >= 0.35`.
- Re-run `make smoke-train` (or the equivalent command listed in `studies/best_commands.md#efficient-smoke-train`) with the high-efficiency flags: bump `--eval-interval` to 5000, set `--eval-samples 1`, raise `--json-chunk-size` to 500+, and enable `--profile-ingest`. Capture the resulting chunk latency/RSS figures in `studies/datasets.md`.
- After the efficient run, export the `var/eval_logs/train-*.json` metrics summary and use it to update `AI_REFERENCE.md` with the new throughput (tokens/s) and probe cadence so the next agent keeps the tuned settings.
- Keep `var/eval_logs/quality_retrain_queue.jsonl` under 200 pending entries by scheduling a focused retrain pass that ingests only that queue once it crosses 150 items, using the `queue-drain` command preset in `studies/best_commands.md`.
